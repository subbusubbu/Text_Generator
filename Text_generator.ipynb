{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01901367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a2183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'C://Users//DELL//Downloads//E.M.Forster.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a96c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f6f07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I\n",
      "The Bertolini\n",
      "\n",
      "\n",
      "“The Signora had no business to do it,” said Miss Bartlett, “no\n",
      "business at all. She promised us south rooms with a view close\n",
      "together, instead of which here are north rooms, looking into a\n",
      "courtyard, and a long way apart. Oh, Lucy!”\n",
      "\n",
      "“And a Cockney, besides!” said Lucy, who had been further saddened by\n",
      "the Signora’s unexpected accent. “It might be London.” She looked at\n",
      "the two rows of English people who were sitting at the table; at the\n",
      "row of white bottles of water \n"
     ]
    }
   ],
   "source": [
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa5c63",
   "metadata": {},
   "source": [
    "# Finding the Unique Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e144541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'à', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'ó', 'ö', 'ù', 'Œ', 'œ', '—', '‘', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding The unique characters in the file\n",
    "unique = sorted(set(text))\n",
    "print(unique)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09906c0e",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "Text Vectorization: Since a neural network cannot process raw string data, we need to assign a numerical value to each character. To achieve this, we can create two dictionaries: one to convert characters into numeric indices and another to translate numeric indices back into characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700ff415",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {u:i for i, u in enumerate(unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f161843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " '*': 8,\n",
       " ',': 9,\n",
       " '-': 10,\n",
       " '.': 11,\n",
       " '0': 12,\n",
       " '1': 13,\n",
       " '2': 14,\n",
       " '3': 15,\n",
       " '4': 16,\n",
       " '5': 17,\n",
       " '6': 18,\n",
       " '7': 19,\n",
       " '8': 20,\n",
       " '9': 21,\n",
       " ':': 22,\n",
       " ';': 23,\n",
       " '=': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " '`': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '£': 82,\n",
       " 'à': 83,\n",
       " 'â': 84,\n",
       " 'ä': 85,\n",
       " 'æ': 86,\n",
       " 'ç': 87,\n",
       " 'è': 88,\n",
       " 'é': 89,\n",
       " 'ê': 90,\n",
       " 'ì': 91,\n",
       " 'í': 92,\n",
       " 'ó': 93,\n",
       " 'ö': 94,\n",
       " 'ù': 95,\n",
       " 'Œ': 96,\n",
       " 'œ': 97,\n",
       " '—': 98,\n",
       " '‘': 99,\n",
       " '’': 100,\n",
       " '“': 101,\n",
       " '”': 102}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d113e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_char = np.array(unique) # Index to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6932d475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0',\n",
       "       '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
       "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
       "       'w', 'x', 'y', 'z', '£', 'à', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê',\n",
       "       'ì', 'í', 'ó', 'ö', 'ù', 'Œ', 'œ', '—', '‘', '’', '“', '”'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07a4b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e123be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 63, 56, ...,  1,  1,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a2f0d",
   "metadata": {},
   "source": [
    "We now have a mapping that we can use to switch between characters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f3e3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter I\\nThe Bertol'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = text[:20]\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1772febf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 63, 56, 71, 75, 60, 73,  1, 34,  0, 45, 63, 60,  1, 27, 60, 73,\n",
       "       75, 70, 67])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dd8b594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 63, 56, 71, 75, 60, 73,  1, 34,  0, 45, 63, 60,  1, 27, 60, 73,\n",
       "       75, 70, 67])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950b558",
   "metadata": {},
   "source": [
    "# Creating Batches\n",
    "\n",
    "In general, our goal is to have the model predict the following character with the best probability given a historical series of characters. The length of the historical series is up to the user. A sequence that is too short leaves us with insufficient information (e.g., given the letter \"a,\" what is the next character), whereas a sequence that is too long causes training to take too long and is likely to overfit to sequence characters that are irrelevant to characters further out. Although there is no ideal sequence length, you should take into account the text itself, the length of typical phrases, and a good understanding of the letters and words that are related to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5456a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I\n",
      "The Bertolini\n",
      "\n",
      "\n",
      "“The Signora had no business to do it,” said Miss Bartlett, “no\n",
      "business at all. She promised us south rooms with a view close\n",
      "together, instead of which here are north rooms, looking into a\n",
      "courtyard, and a long way apart. Oh, Lucy!”\n",
      "\n",
      "“And a Cockney, besides!” said Lucy, who had been further saddened by\n",
      "the Signora’s unexpected accent. “It might be London.” She looked at\n",
      "the two rows of English people who were sitting at the table; at the\n",
      "row of white bottles of water \n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40229fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_line= 'The Signora had no business to do it,” said Miss Bartlett, “no business at all. She promised us south rooms with a view close together, instead of which here are north rooms, looking into a courtyard, and a long way apart. Oh, Lucy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc73c54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stanza_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea4239",
   "metadata": {},
   "source": [
    "So, on an average we can take length of line as 200\n",
    "\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d70c4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7ea565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_seq = len(text)//(seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b5d6c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11857"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905a6d8",
   "metadata": {},
   "source": [
    "These individual character requests are transformed into sequences we can input as a batch using the batch method. Because zero indexing is used, we use seq_len+1. What drop_remainder signifies is as follows:\n",
    "\n",
    "(Optional) Drop remaining items. a tf.bool scalar tf.Tensor that indicates whether the previous batch should be eliminated if it contains less elements than batch_size; the default approach is to keep the larger batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "951ab39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "h\n",
      "a\n",
      "p\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "I\n",
      "\n",
      "\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "B\n",
      "e\n",
      "r\n",
      "t\n",
      "o\n",
      "l\n",
      "i\n",
      "n\n",
      "i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "S\n",
      "i\n",
      "g\n",
      "n\n",
      "o\n",
      "r\n",
      "a\n",
      " \n",
      "h\n",
      "a\n",
      "d\n",
      " \n",
      "n\n",
      "o\n",
      " \n",
      "b\n",
      "u\n",
      "s\n",
      "i\n",
      "n\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "o\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      "”\n",
      " \n",
      "s\n",
      "a\n",
      "i\n",
      "d\n",
      " \n",
      "M\n",
      "i\n",
      "s\n",
      "s\n",
      " \n",
      "B\n",
      "a\n",
      "r\n",
      "t\n",
      "l\n",
      "e\n",
      "t\n",
      "t\n",
      ",\n",
      " \n",
      "“\n",
      "n\n",
      "o\n",
      "\n",
      "\n",
      "b\n",
      "u\n",
      "s\n",
      "i\n",
      "n\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      ".\n",
      " \n",
      "S\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "i\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "u\n",
      "s\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "t\n",
      "h\n",
      " \n",
      "r\n",
      "o\n",
      "o\n",
      "m\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "a\n",
      " \n",
      "v\n",
      "i\n",
      "e\n",
      "w\n",
      " \n",
      "c\n",
      "l\n",
      "o\n",
      "s\n",
      "e\n",
      "\n",
      "\n",
      "t\n",
      "o\n",
      "g\n",
      "e\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      ",\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "e\n",
      "a\n",
      "d\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "c\n",
      "h\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "n\n",
      "o\n",
      "r\n",
      "t\n",
      "h\n",
      " \n",
      "r\n",
      "o\n",
      "o\n",
      "m\n",
      "s\n",
      ",\n",
      " \n",
      "l\n",
      "o\n",
      "o\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      "\n",
      "\n",
      "c\n",
      "o\n",
      "u\n",
      "r\n",
      "t\n",
      "y\n",
      "a\n",
      "r\n",
      "d\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      " \n",
      "l\n",
      "o\n",
      "n\n",
      "g\n",
      " \n",
      "w\n",
      "a\n",
      "y\n",
      " \n",
      "a\n",
      "p\n",
      "a\n",
      "r\n",
      "t\n",
      ".\n",
      " \n",
      "O\n",
      "h\n",
      ",\n",
      " \n",
      "L\n",
      "u\n",
      "c\n",
      "y\n",
      "!\n",
      "”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“\n",
      "A\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      " \n",
      "C\n",
      "o\n",
      "c\n",
      "k\n",
      "n\n",
      "e\n",
      "y\n",
      ",\n",
      " \n",
      "b\n",
      "e\n",
      "s\n",
      "i\n",
      "d\n",
      "e\n",
      "s\n",
      "!\n",
      "”\n",
      " \n",
      "s\n",
      "a\n",
      "i\n",
      "d\n",
      " \n",
      "L\n",
      "u\n",
      "c\n",
      "y\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "o\n",
      " \n",
      "h\n",
      "a\n",
      "d\n",
      " \n",
      "b\n",
      "e\n",
      "e\n",
      "n\n",
      " \n",
      "f\n",
      "u\n",
      "r\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "s\n",
      "a\n",
      "d\n",
      "d\n",
      "e\n",
      "n\n",
      "e\n",
      "d\n",
      " \n",
      "b\n",
      "y\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "S\n",
      "i\n",
      "g\n",
      "n\n",
      "o\n",
      "r\n",
      "a\n",
      "’\n",
      "s\n",
      " \n",
      "u\n",
      "n\n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "c\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      ".\n",
      " \n",
      "“\n",
      "I\n",
      "t\n",
      " \n",
      "m\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "L\n",
      "o\n",
      "n\n",
      "d\n",
      "o\n",
      "n\n",
      ".\n",
      "”\n",
      " \n",
      "S\n",
      "h\n",
      "e\n",
      " \n",
      "l\n",
      "o\n",
      "o\n",
      "k\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "t\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "w\n",
      "o\n",
      " \n",
      "r\n",
      "o\n",
      "w\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "E\n",
      "n\n",
      "g\n",
      "l\n",
      "i\n",
      "s\n",
      "h\n",
      " \n",
      "p\n",
      "e\n",
      "o\n",
      "p\n",
      "l\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "o\n",
      " \n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "t\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      ";\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "\n",
      "\n",
      "r\n",
      "o\n",
      "w\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "t\n",
      "e\n",
      " \n",
      "b\n",
      "o\n",
      "t\n",
      "t\n",
      "l\n",
      "e\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "w\n",
      "a\n",
      "t\n",
      "e\n",
      "r\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Creating  Training Sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "for i in char_dataset.take(500):\n",
    "     print(ind_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a383e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310932f",
   "metadata": {},
   "source": [
    "We now have our sequences, and to produce our target text sequences, we will carry out the following procedures for each of them:\n",
    "\n",
    "Take the input text flow.\n",
    "Shift the input text sequence one step ahead before assigning the target text sequence.\n",
    "Combine them into a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a01b1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37bd8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sequences.map(create_seq_targets)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddb84b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28  63  56  71  75  60  73   1  34   0  45  63  60   1  27  60  73  75\n",
      "  70  67  64  69  64   0   0   0 101  45  63  60   1  44  64  62  69  70\n",
      "  73  56   1  63  56  59   1  69  70   1  57  76  74  64  69  60  74  74\n",
      "   1  75  70   1  59  70   1  64  75   9 102   1  74  56  64  59   1  38\n",
      "  64  74  74   1  27  56  73  75  67  60  75  75   9   1 101  69  70   0\n",
      "  57  76  74  64  69  60  74  74   1  56  75   1  56  67  67  11   1  44\n",
      "  63  60   1  71  73  70  68  64  74  60  59   1  76  74   1  74  70  76\n",
      "  75  63   1  73  70  70  68  74   1  78  64  75  63   1  56   1  77  64\n",
      "  60  78   1  58  67  70  74  60   0  75  70  62  60  75  63  60  73   9\n",
      "   1  64  69  74  75  60  56  59   1  70  61   1  78  63  64  58  63   1\n",
      "  63  60  73  60   1  56  73  60   1  69  70  73  75  63   1  73  70  70\n",
      "  68  74]\n",
      "Chapter I\n",
      "The Bertolini\n",
      "\n",
      "\n",
      "“The Signora had no business to do it,” said Miss Bartlett, “no\n",
      "business at all. She promised us south rooms with a view close\n",
      "together, instead of which here are north rooms\n",
      "\n",
      "\n",
      "[ 63  56  71  75  60  73   1  34   0  45  63  60   1  27  60  73  75  70\n",
      "  67  64  69  64   0   0   0 101  45  63  60   1  44  64  62  69  70  73\n",
      "  56   1  63  56  59   1  69  70   1  57  76  74  64  69  60  74  74   1\n",
      "  75  70   1  59  70   1  64  75   9 102   1  74  56  64  59   1  38  64\n",
      "  74  74   1  27  56  73  75  67  60  75  75   9   1 101  69  70   0  57\n",
      "  76  74  64  69  60  74  74   1  56  75   1  56  67  67  11   1  44  63\n",
      "  60   1  71  73  70  68  64  74  60  59   1  76  74   1  74  70  76  75\n",
      "  63   1  73  70  70  68  74   1  78  64  75  63   1  56   1  77  64  60\n",
      "  78   1  58  67  70  74  60   0  75  70  62  60  75  63  60  73   9   1\n",
      "  64  69  74  75  60  56  59   1  70  61   1  78  63  64  58  63   1  63\n",
      "  60  73  60   1  56  73  60   1  69  70  73  75  63   1  73  70  70  68\n",
      "  74   9]\n",
      "hapter I\n",
      "The Bertolini\n",
      "\n",
      "\n",
      "“The Signora had no business to do it,” said Miss Bartlett, “no\n",
      "business at all. She promised us south rooms with a view close\n",
      "together, instead of which here are north rooms,\n"
     ]
    }
   ],
   "source": [
    "for input_txt, target_txt in  data.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5c6b4",
   "metadata": {},
   "source": [
    "Generating Training Batches\n",
    "\n",
    "To prevent the model from overfitting to specific parts of the text, we'll shuffle these sequences into a random order. This approach ensures that the model can generate characters from any seed text. With the sequences ready, we can now create the training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58d65a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring Batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
    "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = data.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f429daac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(128, 200), dtype=tf.int32, name=None), TensorSpec(shape=(128, 200), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34b1a8",
   "metadata": {},
   "source": [
    "Creating the Model\n",
    "\n",
    "With two LSTM layers and an initial embedding layer, our LSTM-based model will also have a few more properties. The DeepMoji, whose original source code can be accessed here, served as the inspiration for this model's architecture.\n",
    "\n",
    "The input layer will be the embedding layer, which functions as a sort of lookup table that converts each character's numerical indices into a vector with a \"embedding dim\" number of dimensions. As you can expect, the training becomes more difficult as the embedding size increases. This concept is comparable to word2vec, which maps words to an n-dimensional space. Results that have been embedded before being fed directly into the LSTM are typically more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52f0682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finding the Length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ac9920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348375ef",
   "metadata": {},
   "source": [
    "Setting up Loss Function\n",
    "\n",
    "We may import sparse categorical crossentropy from Keras to calculate our loss. Additionally, we'll set this to logits=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0bc2e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b10082e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Defining input shape using Input layer\n",
    "    model.add(Input(shape=(None,), batch_size=batch_size))\n",
    "    \n",
    "    # Embedding layer without batch_input_shape\n",
    "    model.add(Embedding(vocab_size, embed_dim))\n",
    "    \n",
    "    # GRU layer\n",
    "    model.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "    \n",
    "    # Final Dense layer to predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    \n",
    "    # Compilling the model\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assuming vocab_size, embed_dim, rnn_neurons, and batch_size are defined\n",
    "model = create_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    rnn_neurons=rnn_neurons,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76c4e2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,592</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1026</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,361,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">103</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">105,781</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m6,592\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1026\u001b[0m)      │     \u001b[38;5;34m3,361,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m103\u001b[0m)       │       \u001b[38;5;34m105,781\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,473,549</span> (13.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,473,549\u001b[0m (13.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,473,549</span> (13.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,473,549\u001b[0m (13.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8c307",
   "metadata": {},
   "source": [
    "Training the model\n",
    "\n",
    "Before we invest too much time in training, let's make sure our model is happy with everything. To verify that the model now predicts random characters devoid of any training, let's pass in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b113f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 200, 103)  <=== (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "  # Predict off some random batch\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "  # Displaying the dimensions of the predictions\n",
    "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b78cf264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 200, 103), dtype=float32, numpy=\n",
       "array([[[ 1.85161573e-03,  2.64890771e-03, -1.08957419e-03, ...,\n",
       "         -2.39001820e-03, -1.93941919e-03, -9.76242009e-05],\n",
       "        [-1.10793824e-03, -3.12016869e-04,  7.99481058e-05, ...,\n",
       "         -1.89037458e-03, -7.06624612e-03,  3.45381140e-03],\n",
       "        [-2.78524403e-03, -2.67025456e-03,  5.51061705e-04, ...,\n",
       "         -3.06212809e-03, -1.01052644e-02,  3.84708750e-03],\n",
       "        ...,\n",
       "        [ 4.54738643e-03, -2.66303844e-03, -2.29560840e-03, ...,\n",
       "          9.79077071e-04, -8.70374613e-04,  6.12644944e-04],\n",
       "        [ 7.07244128e-03,  4.95957024e-03,  7.28933234e-03, ...,\n",
       "         -8.61400040e-04,  4.97060502e-03,  4.66103759e-03],\n",
       "        [ 8.98629020e-04,  2.95906328e-04,  3.46563593e-03, ...,\n",
       "         -3.35454126e-03, -2.13499833e-03,  4.13955795e-03]],\n",
       "\n",
       "       [[ 3.29102250e-03,  1.29417307e-03,  6.33061770e-03, ...,\n",
       "         -4.37623064e-04, -6.69297122e-04,  6.34765485e-03],\n",
       "        [ 4.79769520e-03, -4.91504092e-03,  5.35190385e-03, ...,\n",
       "         -7.24645564e-04, -4.15598857e-04,  7.53591582e-03],\n",
       "        [ 7.02083344e-03,  2.24122987e-03,  4.88818809e-03, ...,\n",
       "         -2.61366763e-03,  1.78275653e-03,  8.76045972e-03],\n",
       "        ...,\n",
       "        [ 2.98969494e-03, -1.19794707e-03,  1.45932846e-03, ...,\n",
       "          6.12755073e-04, -1.00158146e-02,  9.56362172e-04],\n",
       "        [-3.23148048e-03, -2.85047013e-03,  3.53950565e-03, ...,\n",
       "          1.07447850e-05, -1.52008142e-02,  5.25041018e-03],\n",
       "        [ 1.89252873e-03, -7.87640363e-03,  4.96446388e-03, ...,\n",
       "         -1.03986741e-03, -7.71251041e-03,  6.63564913e-03]],\n",
       "\n",
       "       [[ 2.58332188e-03, -1.29254325e-03,  5.47459349e-04, ...,\n",
       "          4.16076189e-04,  7.68670230e-04,  1.00348622e-03],\n",
       "        [-1.51046738e-03, -5.03269071e-03, -1.54901110e-03, ...,\n",
       "          4.17051930e-03,  2.11840402e-03,  2.18423014e-03],\n",
       "        [ 1.58111437e-03, -2.93224445e-03,  1.20801106e-03, ...,\n",
       "          4.58832411e-03, -8.70519504e-03, -4.12960071e-04],\n",
       "        ...,\n",
       "        [ 2.97679286e-03, -6.51507732e-03,  4.76825330e-03, ...,\n",
       "         -1.91513100e-04, -9.12063755e-03,  5.56575833e-03],\n",
       "        [ 5.55501226e-03,  1.26321567e-03,  5.25137037e-03, ...,\n",
       "         -1.98975485e-03, -2.73660407e-03,  7.78427953e-03],\n",
       "        [-2.23678537e-04, -2.43058754e-03,  6.66000182e-03, ...,\n",
       "         -1.13211619e-03, -1.06718317e-02,  8.54048040e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.27182750e-03,  4.85451147e-03, -2.02870299e-03, ...,\n",
       "         -1.42631819e-03,  5.83047746e-04,  3.01029207e-03],\n",
       "        [-4.66670783e-04,  1.64726691e-03, -8.80704354e-03, ...,\n",
       "         -2.71915132e-03,  9.47017502e-03, -3.70990764e-03],\n",
       "        [ 7.36237620e-04,  3.91968293e-03, -1.66695118e-02, ...,\n",
       "         -7.33718858e-04,  1.05028618e-02, -8.71199556e-03],\n",
       "        ...,\n",
       "        [-2.03503296e-03,  3.90118361e-03, -1.89406716e-03, ...,\n",
       "          1.87233440e-03,  3.09323007e-03, -9.00028972e-05],\n",
       "        [ 2.79306341e-03, -3.67259886e-03,  1.77866011e-03, ...,\n",
       "          2.17140629e-03,  3.23901651e-04,  3.54602654e-03],\n",
       "        [ 4.28551342e-03,  8.54848127e-04, -2.85685004e-04, ...,\n",
       "         -2.72937678e-03, -4.57041850e-03,  3.86805949e-03]],\n",
       "\n",
       "       [[-7.58625683e-04,  5.76092862e-04,  1.96983106e-03, ...,\n",
       "         -5.47594856e-03,  3.51694319e-03,  5.02819754e-03],\n",
       "        [ 2.15174188e-03, -3.56619013e-04, -3.19758616e-03, ...,\n",
       "         -1.37430569e-03, -1.19641202e-03,  5.30216517e-03],\n",
       "        [ 4.49931901e-03,  1.83291850e-03, -4.66173282e-03, ...,\n",
       "         -6.49518473e-03, -5.08161355e-03,  4.97408165e-03],\n",
       "        ...,\n",
       "        [ 5.57152694e-03,  3.38557526e-03, -5.39165922e-03, ...,\n",
       "         -2.65785214e-03, -4.60018497e-03,  4.79208538e-03],\n",
       "        [ 4.04346036e-03,  1.95719372e-03, -1.72899989e-03, ...,\n",
       "          5.94269717e-04, -1.02572050e-02,  2.86713522e-03],\n",
       "        [ 3.25321965e-03, -4.91868658e-03,  1.69607543e-03, ...,\n",
       "          8.04084819e-04, -5.70239499e-03,  5.89690730e-03]],\n",
       "\n",
       "       [[-6.43742969e-04,  6.33056834e-03,  1.63808709e-03, ...,\n",
       "         -4.10334161e-03,  3.71903600e-03,  4.79678670e-03],\n",
       "        [ 4.30856179e-03,  8.02448019e-03,  2.20065308e-03, ...,\n",
       "         -3.79515765e-03,  3.04062990e-03,  6.34842692e-03],\n",
       "        [ 1.93382474e-03,  2.26579281e-03,  6.12595677e-03, ...,\n",
       "          1.25282211e-04, -9.71787726e-04,  3.80625483e-03],\n",
       "        ...,\n",
       "        [ 1.33702648e-03,  5.55449398e-03, -1.87845994e-02, ...,\n",
       "          1.37548102e-03,  1.16116805e-02, -1.09756952e-02],\n",
       "        [ 4.06417344e-03, -4.61364922e-04, -8.95153359e-03, ...,\n",
       "          1.80111546e-03,  5.32276928e-03, -1.28921005e-03],\n",
       "        [ 7.83633033e-04,  5.87308255e-04, -3.59112816e-03, ...,\n",
       "         -3.41293029e-03,  2.03093048e-03,  2.80940160e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e706a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b0664c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200, 1), dtype=int64, numpy=\n",
       "array([[ 37],\n",
       "       [ 69],\n",
       "       [ 14],\n",
       "       [ 96],\n",
       "       [ 70],\n",
       "       [ 33],\n",
       "       [ 24],\n",
       "       [ 75],\n",
       "       [ 22],\n",
       "       [ 83],\n",
       "       [ 40],\n",
       "       [ 27],\n",
       "       [ 90],\n",
       "       [ 43],\n",
       "       [ 72],\n",
       "       [ 33],\n",
       "       [ 86],\n",
       "       [ 70],\n",
       "       [ 50],\n",
       "       [ 71],\n",
       "       [ 61],\n",
       "       [ 47],\n",
       "       [ 48],\n",
       "       [ 96],\n",
       "       [ 61],\n",
       "       [ 21],\n",
       "       [ 34],\n",
       "       [ 25],\n",
       "       [ 74],\n",
       "       [  4],\n",
       "       [ 73],\n",
       "       [ 38],\n",
       "       [ 27],\n",
       "       [  4],\n",
       "       [  0],\n",
       "       [ 16],\n",
       "       [ 49],\n",
       "       [ 10],\n",
       "       [ 34],\n",
       "       [  6],\n",
       "       [ 51],\n",
       "       [ 72],\n",
       "       [ 22],\n",
       "       [ 26],\n",
       "       [ 67],\n",
       "       [ 22],\n",
       "       [  4],\n",
       "       [ 88],\n",
       "       [ 28],\n",
       "       [ 60],\n",
       "       [ 87],\n",
       "       [ 44],\n",
       "       [ 83],\n",
       "       [ 74],\n",
       "       [ 19],\n",
       "       [ 92],\n",
       "       [ 65],\n",
       "       [ 62],\n",
       "       [ 96],\n",
       "       [ 81],\n",
       "       [ 43],\n",
       "       [  7],\n",
       "       [ 30],\n",
       "       [ 69],\n",
       "       [ 62],\n",
       "       [ 59],\n",
       "       [  4],\n",
       "       [ 22],\n",
       "       [ 60],\n",
       "       [ 85],\n",
       "       [101],\n",
       "       [  1],\n",
       "       [ 62],\n",
       "       [ 44],\n",
       "       [ 21],\n",
       "       [ 22],\n",
       "       [ 36],\n",
       "       [ 80],\n",
       "       [ 26],\n",
       "       [ 97],\n",
       "       [ 66],\n",
       "       [ 60],\n",
       "       [ 47],\n",
       "       [ 50],\n",
       "       [  0],\n",
       "       [ 96],\n",
       "       [ 48],\n",
       "       [ 65],\n",
       "       [  6],\n",
       "       [ 44],\n",
       "       [ 61],\n",
       "       [ 14],\n",
       "       [ 63],\n",
       "       [ 23],\n",
       "       [ 80],\n",
       "       [ 12],\n",
       "       [  9],\n",
       "       [ 44],\n",
       "       [102],\n",
       "       [ 43],\n",
       "       [ 90],\n",
       "       [ 43],\n",
       "       [  8],\n",
       "       [  0],\n",
       "       [ 77],\n",
       "       [ 51],\n",
       "       [ 46],\n",
       "       [ 37],\n",
       "       [ 28],\n",
       "       [ 57],\n",
       "       [102],\n",
       "       [ 69],\n",
       "       [ 64],\n",
       "       [ 99],\n",
       "       [ 31],\n",
       "       [ 65],\n",
       "       [ 17],\n",
       "       [ 61],\n",
       "       [ 41],\n",
       "       [ 20],\n",
       "       [  9],\n",
       "       [ 46],\n",
       "       [ 54],\n",
       "       [ 42],\n",
       "       [ 83],\n",
       "       [ 56],\n",
       "       [ 56],\n",
       "       [  3],\n",
       "       [  5],\n",
       "       [ 37],\n",
       "       [ 40],\n",
       "       [ 35],\n",
       "       [ 48],\n",
       "       [ 22],\n",
       "       [ 59],\n",
       "       [ 88],\n",
       "       [  4],\n",
       "       [ 80],\n",
       "       [ 82],\n",
       "       [ 66],\n",
       "       [ 33],\n",
       "       [ 11],\n",
       "       [ 73],\n",
       "       [ 60],\n",
       "       [ 32],\n",
       "       [  9],\n",
       "       [ 31],\n",
       "       [ 83],\n",
       "       [ 44],\n",
       "       [ 75],\n",
       "       [ 92],\n",
       "       [ 54],\n",
       "       [ 93],\n",
       "       [ 29],\n",
       "       [ 10],\n",
       "       [ 38],\n",
       "       [ 96],\n",
       "       [ 42],\n",
       "       [ 37],\n",
       "       [ 42],\n",
       "       [ 77],\n",
       "       [ 45],\n",
       "       [ 79],\n",
       "       [ 39],\n",
       "       [  1],\n",
       "       [  1],\n",
       "       [ 56],\n",
       "       [ 56],\n",
       "       [ 72],\n",
       "       [ 63],\n",
       "       [ 47],\n",
       "       [ 26],\n",
       "       [ 64],\n",
       "       [ 37],\n",
       "       [ 63],\n",
       "       [ 11],\n",
       "       [ 38],\n",
       "       [ 23],\n",
       "       [ 93],\n",
       "       [ 13],\n",
       "       [101],\n",
       "       [  6],\n",
       "       [ 60],\n",
       "       [ 92],\n",
       "       [ 81],\n",
       "       [ 31],\n",
       "       [ 96],\n",
       "       [ 75],\n",
       "       [ 48],\n",
       "       [ 53],\n",
       "       [ 46],\n",
       "       [ 64],\n",
       "       [ 56],\n",
       "       [ 99],\n",
       "       [ 17],\n",
       "       [ 81],\n",
       "       [  7],\n",
       "       [ 96],\n",
       "       [ 26],\n",
       "       [ 18]], dtype=int64)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd3ab073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting to not be a lists of lists\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56c9c8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 37,  69,  14,  96,  70,  33,  24,  75,  22,  83,  40,  27,  90,\n",
       "        43,  72,  33,  86,  70,  50,  71,  61,  47,  48,  96,  61,  21,\n",
       "        34,  25,  74,   4,  73,  38,  27,   4,   0,  16,  49,  10,  34,\n",
       "         6,  51,  72,  22,  26,  67,  22,   4,  88,  28,  60,  87,  44,\n",
       "        83,  74,  19,  92,  65,  62,  96,  81,  43,   7,  30,  69,  62,\n",
       "        59,   4,  22,  60,  85, 101,   1,  62,  44,  21,  22,  36,  80,\n",
       "        26,  97,  66,  60,  47,  50,   0,  96,  48,  65,   6,  44,  61,\n",
       "        14,  63,  23,  80,  12,   9,  44, 102,  43,  90,  43,   8,   0,\n",
       "        77,  51,  46,  37,  28,  57, 102,  69,  64,  99,  31,  65,  17,\n",
       "        61,  41,  20,   9,  46,  54,  42,  83,  56,  56,   3,   5,  37,\n",
       "        40,  35,  48,  22,  59,  88,   4,  80,  82,  66,  33,  11,  73,\n",
       "        60,  32,   9,  31,  83,  44,  75,  92,  54,  93,  29,  10,  38,\n",
       "        96,  42,  37,  42,  77,  45,  79,  39,   1,   1,  56,  56,  72,\n",
       "        63,  47,  26,  64,  37,  63,  11,  38,  23,  93,  13, 101,   6,\n",
       "        60,  92,  81,  31,  96,  75,  48,  53,  46,  64,  56,  99,  17,\n",
       "        81,   7,  96,  26,  18], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd54d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba74512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 5s/step - loss: 3.4820\n",
      "Epoch 2/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 5s/step - loss: 2.2666\n",
      "Epoch 3/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 5s/step - loss: 1.9711\n",
      "Epoch 4/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 5s/step - loss: 1.7488\n",
      "Epoch 5/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 6s/step - loss: 1.5955\n",
      "Epoch 6/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 6s/step - loss: 1.4920\n",
      "Epoch 7/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 5s/step - loss: 1.4226\n",
      "Epoch 8/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 5s/step - loss: 1.3693\n",
      "Epoch 9/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 5s/step - loss: 1.3284\n",
      "Epoch 10/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 4s/step - loss: 1.2962\n",
      "Epoch 11/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 5s/step - loss: 1.2679\n",
      "Epoch 12/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 4s/step - loss: 1.2447\n",
      "Epoch 13/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 5s/step - loss: 1.2231\n",
      "Epoch 14/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 4s/step - loss: 1.2039\n",
      "Epoch 15/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 4s/step - loss: 1.1867\n",
      "Epoch 16/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 5s/step - loss: 1.1644\n",
      "Epoch 17/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 4s/step - loss: 1.1489\n",
      "Epoch 18/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 4s/step - loss: 1.1315\n",
      "Epoch 19/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 4s/step - loss: 1.1154\n",
      "Epoch 20/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 4s/step - loss: 1.0981\n",
      "Epoch 21/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 4s/step - loss: 1.0834\n",
      "Epoch 22/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 4s/step - loss: 1.0662\n",
      "Epoch 23/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 4s/step - loss: 1.0500\n",
      "Epoch 24/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 4s/step - loss: 1.0342\n",
      "Epoch 25/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 4s/step - loss: 1.0206\n",
      "Epoch 26/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 4s/step - loss: 1.0000\n",
      "Epoch 27/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 4s/step - loss: 0.9827\n",
      "Epoch 28/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 4s/step - loss: 0.9650\n",
      "Epoch 29/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 4s/step - loss: 0.9510\n",
      "Epoch 30/30\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 4s/step - loss: 0.9339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27528c7b050>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "10038c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('EM.Forst.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4bce77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1829a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "\n",
    "model.load_weights('EM.Forst.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d7b7e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,592</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1026</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,361,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">103</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">105,781</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m6,592\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1026\u001b[0m)        │     \u001b[38;5;34m3,361,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m103\u001b[0m)         │       \u001b[38;5;34m105,781\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,473,549</span> (13.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,473,549\u001b[0m (13.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,473,549</span> (13.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,473,549\u001b[0m (13.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ce2c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed, gen_size=100, temp=1.0):\n",
    "    '''\n",
    "    model: Trained Model to Generate Text\n",
    "    start_seed: Initial Seed text in string form\n",
    "    gen_size: Number of characters to generate\n",
    "\n",
    "    The basic idea behind this function is to take in some seed text, format it so\n",
    "    that it is in the correct shape for our network, then loop the sequence as\n",
    "    we keep adding our own predicted characters.\n",
    "    '''\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = gen_size\n",
    "\n",
    "    # Vectorizing starting seed text\n",
    "    input_eval = [char_to_ind[s] for s in start_seed]\n",
    "\n",
    "    # Expand to match batch format shape\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty list to hold resulting generated text\n",
    "    text_generated = []\n",
    "\n",
    "    # Temperature effects randomness in our resulting text\n",
    "    temperature = temp\n",
    "\n",
    "    # Here batch size == 1\n",
    "    # Reset states for stateful RNN layers\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'reset_states'):\n",
    "            layer.reset_states()\n",
    "\n",
    "    for i in range(num_generate):\n",
    "\n",
    "        # Generate Predictions\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # Remove the batch shape dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Use a categorical distribution to select the next character\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character for the next input\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        # Transform back to character letter\n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "\n",
    "    return start_seed + ''.join(text_generated)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "31b7499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When she was develop in a dream.\n",
      "\n",
      "Miss Quested, you walked man. Margaret now?”\n",
      "\n",
      "“Yes.”\n",
      "\n",
      "Then, with fire of the r\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,\"When she was\",gen_size=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
